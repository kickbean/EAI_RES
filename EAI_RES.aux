\relax 
\providecommand{\transparent@use}[1]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Picard03}
\citation{Pantic_PAMI00}
\citation{Torre11}
\citation{Torre11}
\citation{FERA11}
\citation{Ekman78}
\citation{Ekman78}
\citation{Bartlett_FG11}
\citation{Valstar_FERA11}
\citation{Fridlund_87}
\citation{Pantic_PAMI00}
\citation{Bartlett_FG11}
\citation{Valstar_SMCB12}
\citation{Yang_SMCB12}
\citation{Dahmane_TMM14}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The overview of our approach. We observe that: 1) a facial expression sequence possess an inherent \textit  {global} model (\textit  {e.g.}, lip corner expansion for \textit  {Happiness} in this case); and 2) each frame has its own \textit  {local} properties (\textit  {e.g.}, various muscle motion intensities), which describe the facial dynamics. Due to the face anatomy constraints\nobreakspace  {}\cite  {Ekman78}, the muscle motion that characterize the local deformation is sparsely distributed. Inspired by these observations, we model each frame as 1) a Low-Rank Expression (LRE) that represents the global expression appearance estimated from the entire sequence; and 2) an additive Sparse Expression Residual (SER) that represents the deviations from the low-rank structure. Moreover, the person-independent factor is modeled as a transformation with respect to a canonical space (reference face), avoiding over-fitting for classification. Both LRE and SER provide viable manners to extract meaningful facial expression dynamics, yielding superior recognition performances. To appreciate the information captured by the LRE and SER representations, the bottom row shows the mean representation of the decomposed sequence. The mean LRE retains the global appearance and the mean SER captures the local motion around mouth and eye areas.}}{1}{figure.1}}
\newlabel{fig:splash}{{1}{1}{The overview of our approach. We observe that: 1) a facial expression sequence possess an inherent \textit {global} model (\textit {e.g.}, lip corner expansion for \textit {Happiness} in this case); and 2) each frame has its own \textit {local} properties (\textit {e.g.}, various muscle motion intensities), which describe the facial dynamics. Due to the face anatomy constraints~\cite {Ekman78}, the muscle motion that characterize the local deformation is sparsely distributed. Inspired by these observations, we model each frame as 1) a Low-Rank Expression (LRE) that represents the global expression appearance estimated from the entire sequence; and 2) an additive Sparse Expression Residual (SER) that represents the deviations from the low-rank structure. Moreover, the person-independent factor is modeled as a transformation with respect to a canonical space (reference face), avoiding over-fitting for classification. Both LRE and SER provide viable manners to extract meaningful facial expression dynamics, yielding superior recognition performances. To appreciate the information captured by the LRE and SER representations, the bottom row shows the mean representation of the decomposed sequence. The mean LRE retains the global appearance and the mean SER captures the local motion around mouth and eye areas}{figure.1}{}}
\citation{Bartlett_FG11}
\citation{Valstar_SMCB12}
\citation{Zhao_PAMI07}
\citation{Huang11}
\citation{Yang_SMCB12}
\citation{Dahmane_TMM14}
\citation{Ekman78}
\citation{Liu_PAMI11}
\citation{Pantic_PAMI00}
\citation{Essa_PAMI97}
\citation{Donato_PAMI99}
\citation{Bartlett_FG11}
\citation{Kaliouby_SMC04}
\citation{Valstar_FERA11}
\citation{Ekman78}
\citation{Whitehill_FG06}
\citation{Lyons_PAMI99}
\citation{Bartlett_FG11}
\citation{Shan_IVC09}
\citation{Jiang_FG11}
\citation{Levi_CVPR04}
\citation{Dalal_CVPR05}
\citation{Dhall_FERA11}
\citation{Dahmane_FERA11}
\citation{LPQ}
\citation{Ekman2005}
\citation{Ambadar05}
\citation{Yacoob_PAMI96}
\citation{Zhao_PAMI07}
\citation{CKplus}
\citation{LPQ-TOP}
\citation{Valstar_SMCB12}
\newlabel{sec:related}{{II}{2}{Related Work and Our Contributions}{section.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work and Our Contributions}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-A}}Facial Expression Recognition}{2}{subsection.2.1}}
\citation{CKplus}
\citation{Valstar_FERA11}
\citation{Valstar12}
\citation{Yang_SMCB12}
\citation{Yang_CVPR08}
\citation{Wright_PAMI09}
\citation{Qiu_ICCV11}
\citation{Zhang_IJCV12}
\citation{Peng_PAMI12}
\citation{Lin12}
\citation{Tariq12}
\citation{Yang_SMCB12}
\citation{Candes11}
\citation{Lin09}
\citation{Peng_CVPR10}
\citation{Lin09}
\citation{Beck09}
\citation{Yang_SMCB12}
\citation{Dahmane_TMM14}
\citation{Liu_PAMI11}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-B}}Sparse Representation}{3}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-C}}Our Contributions}{3}{subsection.2.3}}
\newlabel{sec:decompose}{{III}{3}{Person-independent Expression Modeling}{section.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Person-independent Expression Modeling}{3}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}The Generalized Model}{3}{subsection.3.1}}
\newlabel{eq:org}{{3}{3}{The Generalized Model}{equation.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Reference-based Person-Independent Transformation}{3}{subsection.3.2}}
\citation{Lowe_ICCV99}
\citation{Liu_PAMI11}
\citation{Yang_SMCB12}
\citation{Liu_PAMI11}
\citation{Yang_SMCB12}
\citation{Viola_IJCV04}
\citation{Yang_SMCB12}
\citation{Valstar_FERA11}
\citation{Yang_SMCB12}
\citation{Beck09}
\citation{Beck09}
\citation{Lin09}
\citation{Lin09}
\newlabel{data_term}{{6}{4}{Reference-based Person-Independent Transformation}{equation.3.6}{}}
\newlabel{small_constraint}{{7}{4}{Reference-based Person-Independent Transformation}{equation.3.7}{}}
\newlabel{smooth_constraint}{{8}{4}{Reference-based Person-Independent Transformation}{equation.3.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Reference-based alignment via SIFT-flow\nobreakspace  {}\cite  {Liu_PAMI11} warping. The Avatar Reference is a canonical face representation\nobreakspace  {}\cite  {Yang_SMCB12}. After warping, the head rotation is corrected and person-specific information is attenuated in both sequence.}}{4}{figure.2}}
\newlabel{fig:sift_flow_visual}{{2}{4}{Reference-based alignment via SIFT-flow~\cite {Liu_PAMI11} warping. The Avatar Reference is a canonical face representation~\cite {Yang_SMCB12}. After warping, the head rotation is corrected and person-specific information is attenuated in both sequence}{figure.2}{}}
\newlabel{eq:rpca}{{9}{4}{Reference-based Person-Independent Transformation}{equation.3.9}{}}
\newlabel{eq:apg}{{10}{4}{Reference-based Person-Independent Transformation}{equation.3.10}{}}
\newlabel{eq:apg_fast}{{11}{4}{Reference-based Person-Independent Transformation}{equation.3.11}{}}
\citation{Yang_SMCB12}
\citation{Ojala_PAMI02}
\citation{LPQ}
\citation{Zhao_PAMI07}
\citation{LPQ-TOP}
\citation{libsvm}
\citation{Yang_SMCB12}
\citation{FERA11}
\citation{Yang_SMCB12}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Low-Rank Expression Decomposition}}{5}{algorithm.1}}
\newlabel{alg}{{1}{5}{Reference-based Person-Independent Transformation}{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-C}}Feature Extraction and Classification}{5}{subsection.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The LRE and SER decomposition. A low-rank component and a sparse residual component are jointly estimated and decomposed from a sequence of aligned faces. Although individual frame differs from each other, the mean LRE visually resembles the EAI representation.}}{5}{figure.3}}
\newlabel{fig:low_rank_sparse}{{3}{5}{The LRE and SER decomposition. A low-rank component and a sparse residual component are jointly estimated and decomposed from a sequence of aligned faces. Although individual frame differs from each other, the mean LRE visually resembles the EAI representation}{figure.3}{}}
\newlabel{sec:eai}{{\unhbox \voidb@x \hbox {III-D}}{5}{Relation to Emotion Avatar Image (EAI)}{subsection.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-D}}Relation to Emotion Avatar Image (EAI) }{5}{subsection.3.4}}
\citation{FERA_data}
\citation{CKplus}
\citation{Valstar12}
\citation{Yang_SMCB12}
\citation{Ojala_PAMI02}
\citation{LPQ}
\citation{Valstar12}
\citation{FERA_data}
\citation{Yang_SMCB12}
\citation{Yang_SMCB12}
\citation{Yang_SMCB12}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The comparison of Low-Rank Expressions (LRE) and Emotion Avatar Images (EAI). Although generated from different approaches, their appearances resemble each other at pixel level.}}{6}{figure.4}}
\newlabel{fig:eai_lre_compare}{{4}{6}{The comparison of Low-Rank Expressions (LRE) and Emotion Avatar Images (EAI). Although generated from different approaches, their appearances resemble each other at pixel level}{figure.4}{}}
\newlabel{sec:exp}{{IV}{6}{Experimental Results}{section.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Experimental Results}{6}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Experimental Setup}{6}{subsection.4.1}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces The rank of person-independent test in FERA Challenge\nobreakspace  {}\cite  {Valstar12}}}{6}{table.1}}
\newlabel{table:fera_team}{{I}{6}{The rank of person-independent test in FERA Challenge~\cite {Valstar12}}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Uncontrolled Data: FERA-GEMEP Dataset}{6}{subsection.4.2}}
\newlabel{teams}{{1}{6}{}{Hfootnote.1}{}}
\citation{Yang_SMCB12}
\citation{Valstar12}
\citation{Yang_SMCB12}
\citation{Yang_SMCB12}
\citation{Yang_SMCB12}
\citation{Yang_SMCB12}
\citation{Yang_SMCB12}
\newlabel{fig:lre_vs_id}{{5(a)}{7}{Subfigure 5(a)}{subfigure.5.1}{}}
\newlabel{sub@fig:lre_vs_id}{{(a)}{7}{Subfigure 5(a)\relax }{subfigure.5.1}{}}
\newlabel{fig:ser_vs_id}{{5(b)}{7}{Subfigure 5(b)}{subfigure.5.2}{}}
\newlabel{sub@fig:ser_vs_id}{{(b)}{7}{Subfigure 5(b)\relax }{subfigure.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  The sparse decomposition of facial expression sequence. Each grid is an image representation of a video sequence. (a) The LRE representations. (b) The corresponding SRE representations.}}{7}{figure.5}}
\newlabel{fig:rep_vs_id}{{5}{7}{The sparse decomposition of facial expression sequence. Each grid is an image representation of a video sequence. (a) The LRE representations. (b) The corresponding SRE representations}{figure.5}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {The Low-Rank Expression (LRE) representations}}}{7}{figure.5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {The Sparse Residual Expression (SRE) representations}}}{7}{figure.5}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Classification accuracy on FERA-GEMEP dataset using LBP}}{7}{table.2}}
\newlabel{table:comp_lbp}{{II}{7}{Classification accuracy on FERA-GEMEP dataset using LBP}{table.2}{}}
\citation{CKplus}
\citation{Bartlett03}
\citation{Ekman2005}
\citation{Yang_SMCB12}
\citation{Zhao_PAMI07}
\citation{Zhao_PAMI07}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Classification accuracy on FERA-GEMEP dataset using LPQ}}{8}{table.3}}
\newlabel{table:comp_lpq}{{III}{8}{Classification accuracy on FERA-GEMEP dataset using LPQ}{table.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces confusion matrix for the FERA training dataset using LER+SER. (An=Anger, Fe=Fear, Jo=Joy, Re=Relief, Sa=Sadness)}}{8}{table.4}}
\newlabel{table:mat_fera}{{IV}{8}{confusion matrix for the FERA training dataset using LER+SER. (An=Anger, Fe=Fear, Jo=Joy, Re=Relief, Sa=Sadness)}{table.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The person-independent recognition accuracy with various training size from a fixed subject pool. The feature in use is TOP-LPQ. The performance of LRE is spurred by the additional information captured in SER.}}{8}{figure.6}}
\newlabel{fig:effect_training_size}{{6}{8}{The person-independent recognition accuracy with various training size from a fixed subject pool. The feature in use is TOP-LPQ. The performance of LRE is spurred by the additional information captured in SER}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The boxplot of person-independent recognition accuracy using LRE+SER and TOP-LPQ with various number of people in the training set. The mean performance at the second quartile are connected to illustrate the trend. In general, the performance increases and becomes more stable when the number of training subjects grows.}}{8}{figure.7}}
\newlabel{fig:effect_people_num}{{7}{8}{The boxplot of person-independent recognition accuracy using LRE+SER and TOP-LPQ with various number of people in the training set. The mean performance at the second quartile are connected to illustrate the trend. In general, the performance increases and becomes more stable when the number of training subjects grows}{figure.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-C}}Controlled Data: CK+ dataset}{8}{subsection.4.3}}
\citation{Yang_SMCB12}
\citation{Yang_SMCB12}
\bibstyle{IEEEtran}
\bibdata{expression}
\bibcite{Picard03}{1}
\bibcite{Pantic_PAMI00}{2}
\bibcite{Torre11}{3}
\bibcite{FERA11}{4}
\bibcite{Ekman78}{5}
\bibcite{Bartlett_FG11}{6}
\bibcite{Valstar_FERA11}{7}
\bibcite{Fridlund_87}{8}
\bibcite{Valstar_SMCB12}{9}
\bibcite{Yang_SMCB12}{10}
\bibcite{Dahmane_TMM14}{11}
\bibcite{Zhao_PAMI07}{12}
\bibcite{Huang11}{13}
\bibcite{Liu_PAMI11}{14}
\bibcite{Essa_PAMI97}{15}
\bibcite{Donato_PAMI99}{16}
\bibcite{Kaliouby_SMC04}{17}
\@writefile{lot}{\contentsline {table}{\numberline {V}{\ignorespaces confusion matrix for CK+ dataset using LER+SER. (An=Anger, Co=Contempt, Di=Disgust, Fe=Fear, Ha=Happy, Sa=Sadness, Su=Surprise)}}{9}{table.5}}
\newlabel{table:CK_result}{{V}{9}{confusion matrix for CK+ dataset using LER+SER. (An=Anger, Co=Contempt, Di=Disgust, Fe=Fear, Ha=Happy, Sa=Sadness, Su=Surprise)}{table.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Sample mean LRE representations for the CK+ dataset. The lower right grid shows the Avatar Reference generated from\nobreakspace  {}\cite  {Yang_SMCB12}. Although the person-specific information is attenuated as the appearance of different subjects visually resemble each other, the discriminative expression information is hidden and difficult to be distinguished. In general, they appear to be more ``neutral'' compared with the mean LRE representations of sequences in FERA-GEMEP dataset in Fig.\nobreakspace  {}\ref  {fig:lre_vs_id}.}}{9}{figure.8}}
\newlabel{fig:ck_eai}{{8}{9}{Sample mean LRE representations for the CK+ dataset. The lower right grid shows the Avatar Reference generated from~\cite {Yang_SMCB12}. Although the person-specific information is attenuated as the appearance of different subjects visually resemble each other, the discriminative expression information is hidden and difficult to be distinguished. In general, they appear to be more ``neutral'' compared with the mean LRE representations of sequences in FERA-GEMEP dataset in Fig.~\ref {fig:lre_vs_id}}{figure.8}{}}
\newlabel{sec:conclude}{{V}{9}{Conclusions}{section.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusions}{9}{section.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Relation between the classification accuracy and the training size for each expression category. The accuracy is generally higher for categories with more training data.}}{9}{figure.9}}
\newlabel{fig:fig_ck_size}{{9}{9}{Relation between the classification accuracy and the training size for each expression category. The accuracy is generally higher for categories with more training data}{figure.9}{}}
\@writefile{toc}{\contentsline {section}{References}{9}{section*.1}}
\bibcite{Whitehill_FG06}{18}
\bibcite{Lyons_PAMI99}{19}
\bibcite{Shan_IVC09}{20}
\bibcite{Jiang_FG11}{21}
\bibcite{Levi_CVPR04}{22}
\bibcite{Dalal_CVPR05}{23}
\bibcite{Dhall_FERA11}{24}
\bibcite{Dahmane_FERA11}{25}
\bibcite{LPQ}{26}
\bibcite{Ekman2005}{27}
\bibcite{Ambadar05}{28}
\bibcite{Yacoob_PAMI96}{29}
\bibcite{CKplus}{30}
\bibcite{LPQ-TOP}{31}
\bibcite{Valstar12}{32}
\bibcite{Yang_CVPR08}{33}
\bibcite{Wright_PAMI09}{34}
\bibcite{Qiu_ICCV11}{35}
\bibcite{Zhang_IJCV12}{36}
\bibcite{Peng_PAMI12}{37}
\bibcite{Lin12}{38}
\bibcite{Tariq12}{39}
\bibcite{Candes11}{40}
\bibcite{Lin09}{41}
\bibcite{Peng_CVPR10}{42}
\bibcite{Beck09}{43}
\bibcite{Lowe_ICCV99}{44}
\bibcite{Viola_IJCV04}{45}
\bibcite{Ojala_PAMI02}{46}
\bibcite{libsvm}{47}
\bibcite{FERA_data}{48}
\bibcite{Bartlett03}{49}
